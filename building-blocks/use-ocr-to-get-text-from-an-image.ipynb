{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use OCR to get text from an image (and images from text?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "import tempfile\n",
    "import cv2\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OCR is easy to try, but hard to get right. Sometimes it just works, other times you need to pre-process the images, or adjust the default segmentation settings. At the very least, you probably want to convert colour images to greyscale.\n",
    "\n",
    "In this example we'll load [the image](../images/nla.obj-62330748-1.jpg) into OpenCV (that's the `cv2` prefix), and convert it to greyscale. Then we'll feed it to Tesseract to do the OCR. Because this is a poster, rater than a normal page of text, I've set the page segmentation model (`psm`) to 4, which looks at each line separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('../images/nla.obj-62330748-1.jpg')\n",
    "grey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "text = pytesseract.image_to_string(grey, config='--psm 4')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get images of the letters\n",
    "\n",
    "Pytesseract's `image_to_boxes()` option gives us back individual letters and their bounding boxes. We can then use these bounding boxes to crop images of the letters from the poster. Or at least that's the theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path('letters').mkdir(parents=True, exist_ok=True)\n",
    "h, w, c = img.shape\n",
    "boxes = pytesseract.image_to_boxes(grey, config='--psm 4 --oem 1') \n",
    "for i, b in enumerate(boxes.splitlines()):\n",
    "    b = b.split(' ')\n",
    "    # img = cv2.rectangle(img, (int(b[1]), h - int(b[2])), (int(b[3]), h - int(b[4])), (0, 255, 0), 2)\n",
    "    if b[0].isalpha():\n",
    "        # Note the weird way the coordinates are provided\n",
    "        letter = img[h - int(b[4]):h - int(b[2]), int(b[1]):int(b[3])]\n",
    "        cv2.imwrite(f'letters/{b[0]}-{i}.jpg', letter)                                                  \n",
    "cv2.imwrite('test.jpg', img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look in the `letters` directory to see if it worked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What next?\n",
    "\n",
    "I found the poster I'm using as an example in the `book` zone of Trove. It's part of the ephemera collection. If you'd like to play with more like this, have a look at the [harvesting ephemera](nla.obj-62330748-1.jpg) notebook.\n",
    "\n",
    "What about experimenting with extracting text from other sources? Perhaps digitised files in the National Archives of Australia..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "Created by [Tim Sherratt](https://timsherratt.org/) for the [GLAM Workbench](https://glam-workbench.github.io/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
